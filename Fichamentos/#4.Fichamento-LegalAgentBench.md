LegalAgentBench: Evaluating LLM Agents in the Legal Domain

Referência:
Li, Haitao; Chen, Junjie; et al. "LegalAgentBench: Evaluating LLM Agents in the Legal Domain," arXiv preprint arXiv:2412.17259, Dec. 2024. Disponível em: https://arxiv.org/abs/2412.17259

1. Fichamento de Conteúdo
   
O artigo apresenta o LegalAgentBench, um benchmark abrangente projetado especificamente para avaliar o desempenho de agentes baseados em Large Language Models (LLMs) no domínio jurídico chinês. A motivação é a inadequação dos benchmarks de domínio geral para capturar as complexidades do raciocínio jurídico, que exige múltiplos passos, consulta a bases de conhecimento externas e uso de ferramentas especializadas. A contribuição central é a criação de um ambiente de avaliação robusto, composto por um conjunto de 300 tarefas avaliativas, 17 corpora de dados reais e 37 ferramentas que simulam o workflow de um profissional do direito. Além da métrica de sucesso final, o trabalho introduz o "process rate", que avalia o acerto em passos intermediários do raciocínio do agente. Os resultados dos experimentos com múltiplos LLMs, onde o GPT-4o atingiu a melhor performance com uma taxa de sucesso de 79,08%, demonstram que modelos de fronteira são mais proficientes no uso de ferramentas e em tarefas de raciocínio complexo (multi-hop reasoning).

2. Fichamento Bibliográfico (Glossário de Conceitos)
   
Benchmark (in Software Engineering): Um conjunto padronizado de problemas, tarefas e métricas utilizado para avaliar e comparar o desempenho de diferentes sistemas ou algoritmos de forma objetiva e replicável. O LegalAgentBench é um benchmark de domínio específico, criado para medir a capacidade de agentes de IA em tarefas jurídicas, em contraste com benchmarks gerais como AgentBench.

Multi-hop Reasoning: Um tipo de raciocínio complexo onde a resposta para uma pergunta não pode ser encontrada em uma única fonte de informação, exigindo que o sistema (ou agente) combine informações de múltiplos passos ou documentos para chegar à conclusão correta. O LegalAgentBench inclui tarefas que exigem até 5 passos de raciocínio.

Tool Use (in LLM Agents): A capacidade de um agente de IA de selecionar e utilizar ferramentas externas (como APIs, calculadoras, ou sistemas de busca em banco de dados) para obter informações ou executar ações que não consegue realizar apenas com seu conhecimento interno. O benchmark fornece 37 ferramentas para que os agentes possam interagir com os dados jurídicos.

Process Rate: Uma métrica de avaliação proposta no artigo que, diferentemente da taxa de sucesso final, mede o percentual de passos intermediários que um agente executou corretamente ao resolver uma tarefa. Essa métrica oferece uma visão mais granular sobre onde o raciocínio do agente falha.

3. Fichamento de Citações
   
“We propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain.” (p. 1)

“LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge.” (p. 1)

“Rather than relying solely on final success rates as evaluation criteria, LegalAgentBench introduces the process rate through the annotation of intermediate steps.” (p. 3)

“GPT-4o achieved the best performance with relatively fewer tokens, reaching a success rate of 79.08% under the ReAct method.” (p. 7)

“The uniqueness and complexity of legal judgment require rich expertise and human insight, qualities that LLM agents
