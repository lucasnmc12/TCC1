LegalAgentBench: Evaluating LLM Agents in the Legal Domain

Li, Haitao; Chen, Junjie; Yang, Jingli; Ai, Qingyao; Wei, Jia; Liu, Youfeng; Lin, Kai; Wu, Yueyue; Yuan, Guozhi; Hu, Yiran; Wang, Wuyue; Liu, Yiqun; Huang, Minlie. "LegalAgentBench: Evaluating LLM Agents in the Legal Domain," arXiv preprint arXiv:2412.17259, Dec. 2024. Disponível em: https://arxiv.org/abs/2412.17259
 
1. Fichamento de Conteúdo

O artigo apresenta o LegalAgentBench, um benchmark especificamente desenvolvido para avaliar agentes baseados em LLM no domínio jurídico chinês. O problema identificado é que benchmarks gerais (como AgentBench e ToolBench) não capturam a complexidade das tarefas jurídicas reais, que envolvem múltiplos passos, uso de bases normativas e raciocínio especializado. Para suprir essa lacuna, os autores criaram um conjunto com 17 corpora reais (bancos de dados de empresas, cortes, casos e artigos legais) e 37 ferramentas que permitem interação com esses dados. Além disso, desenvolveram 300 tarefas com diferentes níveis de dificuldade, incluindo raciocínio multi-hop e redação de peças jurídicas, avaliadas não apenas pelo resultado final, mas também pelo progresso em passos intermediários. Os experimentos mostram que modelos avançados como GPT-4o, Claude e Qwen têm melhor desempenho em tarefas complexas, enquanto modelos menores falham em usar ferramentas de forma eficaz. O trabalho destaca limitações atuais, como dificuldades dos LLMs em lidar com terminologia jurídica e em integrar jurisprudência de forma adequada, mas abre caminho para pesquisas que visem melhorar a aplicabilidade dos agentes no setor jurídico.

2. Fichamento Bibliográfico

- Benchmark jurídico especializado — LegalAgentBench é o primeiro dataset amplo para avaliar LLM-agents no contexto jurídico chinês, com foco em cenários autênticos (p. 2–3)
- Diversidade de tarefas — inclui 300 tarefas, desde consultas simples até raciocínio multi-hop de 5 passos e elaboração de documentos (p. 5)
- Ferramentas integradas — 37 ferramentas (retrievers, consultas a bases tabulares, cálculos, etc.) permitem simulação de workflows de advogados (p. 3–4)
- Métricas refinadas — além da taxa de sucesso final, introduz o “process rate” que mede acertos em etapas intermediárias, oferecendo avaliação mais granular (p. 6)
- Limitações — dataset restrito ao sistema jurídico chinês e, por ora, apenas em mandarim; necessidade de adaptação para outros ordenamentos (p. 11)


3. Fichamento de Citações

- “We propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain.” (p. 1)
- “LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge.” (p. 1)
- “Rather than relying solely on final success rates as evaluation criteria, LegalAgentBench introduces the process rate through the annotation of intermediate steps.” (p. 3)
- “GPT-4o achieved the best performance with relatively fewer tokens, reaching a success rate of 79.08% under the ReAct method.” (p. 7)
- “The uniqueness and complexity of legal judgment require rich expertise and human insight, qualities that LLM agents cannot fully replace.” (p. 11)